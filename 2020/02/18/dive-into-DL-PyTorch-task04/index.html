<!DOCTYPE html>
<!--                                                                         
           .,:,,,                                        .::,,,::.          
         .::::,,;;,                                  .,;;:,,....:i:         
         :i,.::::,;i:.      ....,,:::::::::,....   .;i:,.  ......;i.        
         :;..:::;::::i;,,:::;:,,,,,,,,,,..,.,,:::iri:. .,:irsr:,.;i.        
         ;;..,::::;;;;ri,,,.                    ..,,:;s1s1ssrr;,.;r,        
         :;. ,::;ii;:,     . ...................     .;iirri;;;,,;i,        
         ,i. .;ri:.   ... ............................  .,,:;:,,,;i:        
         :s,.;r:... ....................................... .::;::s;        
         ,1r::. .............,,,.,,:,,........................,;iir;        
         ,s;...........     ..::.,;:,,.          ...............,;1s        
        :i,..,.              .,:,,::,.          .......... .......;1,       
       ir,....:rrssr;:,       ,,.,::.     .r5S9989398G95hr;. ....,.:s,      
      ;r,..,s9855513XHAG3i   .,,,,,,,.  ,S931,.,,.;s;s&BHHA8s.,..,..:r:     
    :r;..rGGh,  :SAG;;G@BS:.,,,,,,,,,.r83:      hHH1sXMBHHHM3..,,,,.ir.    
   ,si,.1GS,   sBMAAX&MBMB5,,,,,,:,,.:&8       3@HXHBMBHBBH#X,.,,,,,,rr    
   ;1:,,SH:   .A@&&B#&8H#BS,,,,,,,,,.,5XS,     3@MHABM&59M#As..,,,,:,is,   
   .rr,,,;9&1   hBHHBB&8AMGr,,,,,,,,,,,:h&&9s;   r9&BMHBHMB9:  . .,,,,;ri.  
   :1:....:5&XSi;r8BMBHHA9r:,......,,,,:ii19GG88899XHHH&GSr.      ...,:rs.  
   ;s.     .:sS8G8GG889hi.        ....,,:;:,.:irssrriii:,.        ...,,i1,  
   ;1,         ..,....,,isssi;,        .,,.                      ....,.i1,  
   ;h:               i9HHBMBBHAX9:         .                     ...,,,rs,  
   ,1i..            :A#MBBBBMHB##s                             ....,,,;si.  
   .r1,..        ,..;3BMBBBHBB#Bh.     ..                    ....,,,,,i1;   
    :h;..       .,..;,1XBMMMMBXs,.,, .. :: ,.               ....,,,,,,ss.   
     ih: ..    .;;;, ;;:s58A3i,..    ,. ,.:,,.             ...,,,,,:,s1,    
     .s1,....   .,;sh,  ,iSAXs;.    ,.  ,,.i85            ...,,,,,,:i1;     
      .rh: ...     rXG9XBBM#M#MHAX3hss13&&HHXr         .....,,,,,,,ih;      
       .s5: .....    i598X&&A&AAAAAA&XG851r:       ........,,,,:,,sh;       
       . ihr, ...  .         ..                    ........,,,,,;11:.       
          ,s1i. ...  ..,,,..,,,.,,.,,.,..       ........,,.,,.;s5i.         
           .:s1r,......................       ..............;shs,           
           . .:shr:.  ....                 ..............,ishs.             
               .,issr;,... ...........................,is1s;.               
                  .,is1si;:,....................,:;ir1sr;,                  
                     ..:isssssrrii;::::::;;iirsssssr;:..                    
                          .,::iiirsssssssssrri;;:.                      
 			-->


<script src="http://libs.baidu.com/jquery/2.0.0/jquery.min.js"></script>
<script type="text/javascript" src="/js/JSClick.js"></script>
<script src="/live2d-widget/autoload.js"></script>













  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
<script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/mei.png?v=7.1.1">



  <link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222">


  <link rel="manifest" href="/images/manifest.json">






<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="简介ElitesAI·动手学深度学习PyTorch版《动手学深度学习PyTorch版》在线书籍本部分为Task04：机器翻译及相关技术；注意力机制与Seq2seq模型;Transformer，文字处理部分我还是比较薄弱的，这一部分还是得仔细思考下。。 机器翻译及相关技术机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。主要特征：输出是">
<meta name="keywords" content="《动手学深度学习》">
<meta property="og:type" content="article">
<meta property="og:title" content="《动手学》打卡挑战-task04">
<meta property="og:url" content="http://yoursite.com/2020/02/18/dive-into-DL-PyTorch-task04/index.html">
<meta property="og:site_name" content="I&#39;m LaplaTom">
<meta property="og:description" content="简介ElitesAI·动手学深度学习PyTorch版《动手学深度学习PyTorch版》在线书籍本部分为Task04：机器翻译及相关技术；注意力机制与Seq2seq模型;Transformer，文字处理部分我还是比较薄弱的，这一部分还是得仔细思考下。。 机器翻译及相关技术机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。主要特征：输出是">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jchqoppn.png?imageView2/0/w/440/h/440">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800">
<meta property="og:image" content="https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960">
<meta property="og:updated_time" content="2020-02-18T09:25:59.064Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《动手学》打卡挑战-task04">
<meta name="twitter:description" content="简介ElitesAI·动手学深度学习PyTorch版《动手学深度学习PyTorch版》在线书籍本部分为Task04：机器翻译及相关技术；注意力机制与Seq2seq模型;Transformer，文字处理部分我还是比较薄弱的，这一部分还是得仔细思考下。。 机器翻译及相关技术机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。主要特征：输出是">
<meta name="twitter:image" content="https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640">



  <link rel="alternate" href="/atom.xml" title="I'm LaplaTom" type="application/atom+xml">



  
  
  <link rel="canonical" href="http://yoursite.com/2020/02/18/dive-into-DL-PyTorch-task04/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>《动手学》打卡挑战-task04 | I'm LaplaTom</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">I'm LaplaTom</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">LaplaTom的心灵小屋</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/18/dive-into-DL-PyTorch-task04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LCQ">
      <meta itemprop="description" content="这是LCQ的个人小网站，然后。。没了= =">
      <meta itemprop="image" content="/images/mei.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="I'm LaplaTom">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">《动手学》打卡挑战-task04

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-18 16:22:03 / 修改时间：17:25:59" itemprop="dateCreated datePublished" datetime="2020-02-18T16:22:03+08:00">2020-02-18</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><a href="https://www.boyuai.com/elites/course/cZu18YmweLv10OeV" target="_blank" rel="noopener">ElitesAI·动手学深度学习PyTorch版</a><br><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">《动手学深度学习PyTorch版》在线书籍</a><br>本部分为Task04：<code>机器翻译及相关技术</code>；<code>注意力机制与Seq2seq模型</code>;<code>Transformer</code>，文字处理部分我还是比较薄弱的，这一部分还是得仔细思考下。。</p>
<h3 id="机器翻译及相关技术"><a href="#机器翻译及相关技术" class="headerlink" title="机器翻译及相关技术"></a>机器翻译及相关技术</h3><p><strong>机器翻译（MT）：</strong>将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。<br><strong>主要特征：</strong>输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。</p>
<h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h4><p> encoder：输入到隐藏状态<br> decoder：隐藏状态到输出<br><img src="https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<h4 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h4><h5 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h5><p>训练<br><img src="https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640" alt="Image Name"><br>预测<br><img src="https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<h5 id="具体结构："><a href="#具体结构：" class="headerlink" title="具体结构："></a>具体结构：</h5><p><img src="https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500" alt="Image Name"></p>
<h4 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h4><p>简单greedy search：<br><img src="https://cdn.kesci.com/upload/image/q5jchqoppn.png?imageView2/0/w/440/h/440" alt="Image Name"><br>维特比算法：选择整体分数最高的句子（搜索空间太大）<br>集束搜索：<br><img src="https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<h4 id="PyTorch代码"><a href="#PyTorch代码" class="headerlink" title="PyTorch代码"></a>PyTorch代码</h4><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>在“编码器—解码器（seq2seq）”⼀节⾥，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。</p>
<p>与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。</p>
<p><img src="https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960" alt="Image Name"><br>注意力机制</p>
<ul>
<li>注意力层显式地选择相关的信息。</li>
<li>注意层的内存由键-值对组成，因此它的输出接近于键类似于查询的值。</li>
</ul>
<h4 id="注意力机制框架"><a href="#注意力机制框架" class="headerlink" title="注意力机制框架"></a>注意力机制框架</h4><p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。</p>
<p>为了计算输出，我们首先假设有一个函数$\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \ldots, a_n$ by</p>
<script type="math/tex; mode=display">
a_i = \alpha(\mathbf q, \mathbf k_i).</script><p>我们使用 softmax函数 获得注意力权重：</p>
<script type="math/tex; mode=display">
b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).</script><p>最终的输出就是value的加权求和：</p>
<script type="math/tex; mode=display">
\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.</script><p><img src="https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960" alt="Image Name"><br>不同的attetion layer的区别在于score函数的选择。</p>
<h4 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h4><p>The dot product 假设query和keys有相同的维度, 即 $\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下</p>
<script type="math/tex; mode=display">
𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \sqrt{d}</script><p>假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：</p>
<script type="math/tex; mode=display">
𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\sqrt{d}</script><h4 id="多层感知机注意力"><a href="#多层感知机注意力" class="headerlink" title="多层感知机注意力"></a>多层感知机注意力</h4><p>在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射<br>$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义</p>
<script type="math/tex; mode=display">
𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)</script><p>.<br>然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p>
<h4 id="引入注意力机制的Seq2seq模型"><a href="#引入注意力机制的Seq2seq模型" class="headerlink" title="引入注意力机制的Seq2seq模型"></a>引入注意力机制的Seq2seq模型</h4><p>将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的$t$时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入$D_t$拼接起来一起送到解码器：<br><img src="https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800" alt="Image Name"></p>
<script type="math/tex; mode=display">
Fig1具有注意机制的seq-to-seq模型解码的第二步</script><p>下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构<br><img src="https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800" alt="Image Name"></p>
<script type="math/tex; mode=display">
Fig2具有注意机制的seq-to-seq模型中层结构</script><h4 id="PyTorch代码-1"><a href="#PyTorch代码-1" class="headerlink" title="PyTorch代码"></a>PyTorch代码</h4><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：</p>
<ul>
<li>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</li>
<li>RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。</li>
</ul>
<p>为了整合CNN和RNN的优势，<a href="https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017" target="_blank" rel="noopener">[Vaswani et al., 2017]</a> 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。</p>
<p>图10.3.1展示了Transformer模型的架构，与9.7节的seq2seq模型相似，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p>
<ol>
<li>Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li>
<li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。</li>
<li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li>
</ol>
<p><img src="https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960" alt="Fig. 10.3.1 The Transformer architecture."></p>
<script type="math/tex; mode=display">
Fig.10.3.1\ Transformer 架构.</script><p> <strong><em>具体之后详细写</em></strong> </p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/《动手学深度学习》/" rel="tag"># 《动手学深度学习》</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/02/18/dive-into-DL-PyTorch-task03/" rel="next" title="《动手学》打卡挑战-task03">
                <i class="fa fa-chevron-left"></i> 《动手学》打卡挑战-task03
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/18/dive-into-DL-PyTorch-task05/" rel="prev" title="《动手学》打卡挑战-task05">
                《动手学》打卡挑战-task05 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/mei.png" alt="LCQ">
            
              <p class="site-author-name" itemprop="name">LCQ</p>
              <div class="site-description motion-element" itemprop="description">这是LCQ的个人小网站，然后。。没了= =</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/iRotten" title="GitHub &rarr; https://github.com/iRotten" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:1807318089@qq.com" title="E-Mail &rarr; mailto:1807318089@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/images/QQ.jpg" title="QQ &rarr; /images/QQ.jpg"><i class="fa fa-fw fa-globe"></i>QQ</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://space.bilibili.com/15268664" title="Bilibili &rarr; https://space.bilibili.com/15268664" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>Bilibili</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器翻译及相关技术"><span class="nav-number">2.</span> <span class="nav-text">机器翻译及相关技术</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-number">2.1.</span> <span class="nav-text">Encoder-Decoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sequence-to-Sequence模型"><span class="nav-number">2.2.</span> <span class="nav-text">Sequence to Sequence模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#模型："><span class="nav-number">2.2.1.</span> <span class="nav-text">模型：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#具体结构："><span class="nav-number">2.2.2.</span> <span class="nav-text">具体结构：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Beam-Search"><span class="nav-number">2.3.</span> <span class="nav-text">Beam Search</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PyTorch代码"><span class="nav-number">2.4.</span> <span class="nav-text">PyTorch代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#注意力机制"><span class="nav-number">3.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#注意力机制框架"><span class="nav-number">3.1.</span> <span class="nav-text">注意力机制框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#点积注意力"><span class="nav-number">3.2.</span> <span class="nav-text">点积注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多层感知机注意力"><span class="nav-number">3.3.</span> <span class="nav-text">多层感知机注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#引入注意力机制的Seq2seq模型"><span class="nav-number">3.4.</span> <span class="nav-text">引入注意力机制的Seq2seq模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PyTorch代码-1"><span class="nav-number">3.5.</span> <span class="nav-text">PyTorch代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer"><span class="nav-number">4.</span> <span class="nav-text">Transformer</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LCQ</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.1"></script>

  <script src="/js/motion.js?v=7.1.1"></script>



  
  


  <script src="/js/affix.js?v=7.1.1"></script>

  <script src="/js/schemes/pisces.js?v=7.1.1"></script>




  
  <script src="/js/scrollspy.js?v=7.1.1"></script>
<script src="/js/post-details.js?v=7.1.1"></script>



  


  <script src="/js/next-boot.js?v=7.1.1"></script>


  

  

  

  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  

  


  <script type="text/javascript" src="/js/canvas-nest.js"></script>
</body>
</html>

{"pages":[{"title":"分类","text":"","link":"/categories/index.html"}],"posts":[{"title":"【学习笔记】week1-cnn","text":"###写在之前我之所以开这个博客就是为了记一下我个人的学习笔记，因为最近学的信息量太多，小脑瓜又不一定记得住，所以写了个弄个这个博客。。。。言归正传，我在第一周主要了解CNN的大概模型，学着学着发现缺的非常之多，所以今天这个主要是写CNN（convolution neural network，卷积神经网络），主要是以李宏毅的深度学习视频为主，还参考了网上大佬的学习笔记，另外，本周只是个大概，具体的实验结果和细节以及数学公式之后在说= = 卷积神经网络（CNN）卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一，被大量应用于计算机视觉、图片处理等领域。 CNN的基本结构网络的一般结构为： 输入层 -&gt; 卷积层 -&gt; 激活函数 -&gt; 池化层 -&gt; 全连接层 -&gt; 输出层 输入层(INPUT)：顾名思义，用于输入数据 卷积层(CONV)：使用卷积核对样本数据进行扫描，提取特征值和特征映射 激活函数(RELU)：归一化识别高频特征 池化层(POOL)：对卷积后的特征图进行采样处理，减少数据运算量 全连接层(FC)：进行重新拟合，减少特征信息的损失 输出层(OUTPUT)：用于输出结果CNN的各层解析1.卷积层卷积说白了其实就是一种神经网络的连接方式，就是因为这种卷积的方式，才使得CNN的参数要比全连接神经网络的参数少得多。 （1）对于黑白图片，其通道数为1，其卷积方式如下：PS(个人注解)：在第一图中，卷积核的权值是自由确定的，因为在训练过程中，模型会自行对权值进行修改，根据反向传播（之后会写的），自定义的大小会影响调整的速度。 （2）对于彩色图片，其通道数为3（RGB三原色），其卷积方式如下：PS(个人注解)：彩色的卷积方式和黑白的本质上是一样的，就是拿卷积核进行扫描，然后因为是彩色的，具有RBG三通道，故卷积核也要是三层的（其目的是为了同时对RGB同时扫描），具体的卷积过程可以参考本网址= =补0扫描，步数为2 （3）卷积和全连接方式的对比，体现出利用卷积的连接方式比全连接方式所需的参数更少。（我差点忘了李宏毅讲过这里= - =）刚开始看这图其实我并不是很了解的= =，然后看到下面这图，回想起李宏毅的话，我终于了解卷积核全连接的区别了= =刚开始我是当做这是讲卷积的代码实现思路来着= =然后看了大佬的博客有点小明白了= =所以代码实现我。。还是看JAVA去吧- - （4）卷积层和激励层通常合并在一起称为“卷积层”激励层，我们俗称“激活函数”，它主要对卷积之后的结果进行一个非线性映射，说白了就是讲卷积之后的输出进行函数运算使其值在[01]区间内，我们一般使用RELU函数进行运算，基本上激励层是在卷积之后进行（但李宏毅貌似没讲= =还是忘了。。PS：函数的运算使其在[01]之间，称为“归一化”。其就是找出高频特征。 2.池化层池化层又称采样层，它的作用是减小数据处理量同时保留有用信息 当输入经过卷积层时，若感受视野比较小（就是卷积核比较小），步长stride（步长代表着卷积核一次移动几格，基本是为1)比较小，得到的feature map （特征图）还是比较大，可以通过池化层来对每一个 feature map 进行降维操作，输出的深度（这应该说是维度的深度，即扫描之后有几张图）还是不变的，依然为 feature map 的个数。 池化层也有一个“池化视野（filter）”来对feature map矩阵进行扫描，对“池化视野”中的矩阵值进行计算，一般有两种计算方式： Max pooling：取“池化视野”矩阵中的最大值我是尺寸 Average pooling：取“池化视野”矩阵中的平均值 以Max pooling为例，见下图：PS（个人注解）：采样核的定义可以自己定义，在JAVA当中必须能整除，而李宏毅视频中可以不整除，这个需注意~ 3.全连接层作用：把所有局部特征结合变成全局特征，用来计算最后每一类的得分换句话说全连接层就是把卷积层和池化层的输出展开成一维形式，在后面接上与普通网络结构相同的回归网络或者分类网络，一般接在池化层后面，如图所示;没啥好说的，就是拉直后通过分类网络将其分为所需要的几个类，通过输出层根据softmax函数啥的输出= = 具体代码啥的对于手写数字识别，我发现GitHub有个例子，点此，现场体验。接下来是一些有用的图片= =PS(个人注解)：这是计算神经元的个数，在convolution2D(x,y,z)中，y,z分别代表卷积核的长和宽，x代表扫描的维度（可以认为一次扫了几遍），需注意的是x容易被忽视= = CNN的主要特点（算了，有总比没有好= =这里主要讨论CNN相比与传统的神经网络的不同之处，CNN主要有三大特色，分别是局部感知、权重共享和多卷积核 1.局部感知局部感知就是我们上面说的感受野，实际上就是卷积核和图像卷积的时候，每次卷积核所覆盖的像素只是一小部分，是局部特征，所以说是局部感知。CNN是一个从局部到整体的过程（局部到整体的实现是在全连通层），而传统的神经网络是整体的过程。 2.权重共享传统的神经网络的参数量是非常巨大的，比如1000X1000像素的图片，映射到和自己相同的大小，需要（1000X1000）的平方，也就是10的12次方，参数量太大了，而CNN除全连接层外，卷积层的参数完全取决于滤波器的设置大小，比如10x10的滤波器，这样只有100个参数，当然滤波器的个数不止一个，也就是下面要说的多卷积核。但与传统的神经网络相比，参数量小，计算量小。整个图片共享一组滤波器的参数。 3.多卷积核一种卷积核代表的是一种特征，为获得更多不同的特征集合，卷积层会有多个卷积核，生成不同的特征，这也是为什么卷积后的图片的高，每一个图片代表不同的特征。","link":"/2019/05/25/week1-cnn/"},{"title":"scoop快速开始","text":"写在之前众所周知，在Windows下安装各种软件都是非常非常非常非常麻烦的，而且用学校的网，就算你挂着某个东西，你也不一定能下载下来。由于生活所需，朕苦于安装anaconda，心态都被被打崩了，这时hao哥给我推荐了scoop，用完之后，感觉良好，坑踩完了之后写着记一下，以便之后不会在踩坑。 安装条件首先你要进入powershell界面，没错就是那个PS开头的那个界面，你要确保你的powershell版本是大于等于3的，你可以输入这串代码来查看你的版本：$psversiontable.psversion.major # should be &gt;= 3确认你已经允许powershell执行本地脚本：et-executionpolicy remotesigned -scope currentuser 安装scoop在powershell中运行：iex (new-object net.webclient).downloadstring('https://get.scoop.sh')如果你要自定义安装目录可以(假设目标目录为 C:\\scoop)：$env:SCOOP='C:\\scoop' [environment]::setEnvironmentVariable('SCOOP',$env:SCOOP,'User') iex (new-object net.webclient).downloadstring('https://get.scoop.sh')这样子基本上应该是success了，如果有报错自行百度= = 使用scoopscoop这种傻瓜型软件你可以在help界面看到大部分操作清单，调用方式为:scoop help你若要搜索软件可以直接：scoop search &lt;软件名&gt;安装软件的话：scoop install &lt;软件名&gt;如果提示Couldn’t find manifest for ‘&lt;软件名&gt;’.这时候就需要用bucket，Manage Scoop buckets，是管理可以用scoop下载APP的列表，因为scoop自带的下载APP比较少一些其他第三方的软件需要添加bucket中，scoop提供了一个extras的app列表，来提供更多常用的软件下载，用如下方法添加：scoop bucket add extras https://github.com/lukesampson/scoop-extras.git添加成功后就能下载软件了scoop install extras/anaconda若你要自定义安装软件的位置的话，可以选择这个方法(假设安装目录为：C:\\apps)： $env:SCOOP_GLOBAL='c:\\apps' [environment]::setEnvironmentVariable('SCOOP_GLOBAL',$env:SCOOP_GLOBAL,'Machine') scoop install -g &lt;软件名&gt;PS:scoop安装完成后会自动帮你安装和配好环境，那是相当的厉害。","link":"/2019/05/18/scoop/"},{"title":"【学习笔记】week2-基础","text":"写在之前这次主要是基础，以及一些数学公式= =，参照《卷积神经网络的Python实现》以及李宏毅的视频~~ = 正文 = 批次（batch）定义：批量更新的大小，直白的说就是每次训练是拿多少个样本进行训练。 当数据集较小时，可以采用 全数据集（Full Batch Learning） 的形式，这样训练的方向代表着样本总体的方向，使训练结果更加准确；并且Full Batch Learning可以使用弹性反向传播算法(RProp) 只基于梯度符号并且针对性单独更新因为不同权重的梯度值导致的权值。当数据集较大时，以上的好处便不存在了；其一，随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。其二，以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来均方根反向传播(RMSProp) 的妥协方案。 弹性反向传播(RProp)和均方根反向传播(RMSProp)都是一种权值更新算法，类似于SGD算法，其中，RMSProp是RProp算法的改良版。（自觉百度）当Batch_Size==1时当每次只训练一次样本时，即Batch_Size = 1。这就是在线学习（Online Learning） 。训练的结果难以达到收敛，其原因是Online Leaning训练的每一个样本都存在各自的特性性，不能代表其种类的特征性，导致在训练的时候朝着各自的方向修正，效果如图所示 当Batch_Size盲目增大，又不等于全部时1.内存利用率提高了，但是内存容量可能撑不住了。2.跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。3.Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。 当Batch_Size为合理大小时1.内存利用率提高了，大矩阵乘法的并行化效率提高。2.跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。3.在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小 以上总结：这就是批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。 损失函数定义损失函数（loss function，也叫代价函数，cost function）用来评价分值向量的好坏。分值向量与真实标签之间的差异越大，损失函数值就越大，反之则越小。损失函数是深度学习的核心部分，通过它，可以将所要求的参数调整为需要的大小，其数学公式为: x1=x0-L_R*L_F(x0) (其中x0，x1分别是调整前后的值，L_R为学习率，L_F为损失函数)常用的损失函数而目前我们常用的损失函数有以上来自此博客（md的数学公式是真的烦，我之后在弄，顺便把图像也弄一下） 激活函数定义根据奥卡姆剃刀法则，神经网络采用的是最简单的线性模型 f(x) = wx+b, 并且线性模型的组合仍然为线性模型。但我们要解决的大部分问题又不仅仅是一个线性问题，因此在线性模型中增加非线性元素就显得尤其重要了。激活函数正是增加非线性的一个重要手段。 常用的激活函数sigmoid函数函数表达式为sigmoid是早期使用较多的激活函数，优点为 输出值为[0, 1], 符合概率分布 x靠近0时斜率很大，对应为神经元兴奋区，远离0的区域斜率很小，对应为神经元抑制区 但其缺点也较为明显 x稍微远离0, 导数就接近于0了，这样在反向传播优化w时，无论w取何值，梯度都很小，也就无法对w更新给出指导了，这种现象称为梯度弥散，在网络层级较深时尤其明显。 需要进行指数计算，比较耗时 容易造成梯度爆炸和梯度消失的结果 tanh函数表达式为与sigmoid类似，只是Y值的分布是以0为中心，其问题也存在。 RELU函数表达式为relu是当前使用最为频繁的激活函数，在几乎所有的CNN网络均使用relu作为激活函数，它解决了sigmoid和tanh的问题，优点为 x大于0时，导数为1，在反向传播计算梯度时，不存在梯度弥散问题。 x&gt;0时，y=x, x&lt;0时，y=0, 计算十分简单。 针对于relu的x&lt;0部分，出现了一些变种函数，比如ELU和PRELU，均是针对x&lt;0分支的改进，和relu大体相似。 学习率（Learning Rate）学习率是指导我们该如何通过损失函数的梯度调整网络权重的超参数。学习率越低，损失函数的变化速度就越慢。虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在高原区域的情况下。 下述公式表示了上面所说的这种关系。new_weight = existing_weight — learning_rate * gradient 当学习率过小或者学习率过大都会对训练的情况造成很大的影响，如图：当学习率过小时，虽然能达到拟合，但需要的时间特别长；当学习率过大时，可能收敛不了，故学习率应适中。在实际操作情况下，一般学习率先大后小，先快速下降，后通过减小学习率达到收敛。参考机器之心 ####学习率优化算法学习率优化算法，在卷积神经网络一书中主要介绍了以下算法： 随机梯度下降法 动量法 AdaGrad 。。。。 具体看书吧= = 反向传播没啥好讲的，不是很了解= =这里","link":"/2019/06/01/week2/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/05/08/hello-world/"},{"title":"intellij-idea快捷键","text":"个人常用 Ctrl+B 快速打开光标处的类或方法 Ctrl+F11 数字/默认书签 Ctrl+Alt+ left/right 返回至上次浏览的位置 CTRL+E 最近更改的代码 Ctrl+Alt+h：显示调用当前方法的所有位置。 CTRL+G 定位行 以下来自http://www.open-open.com/lib/view/open1396578860887.html ———–自动代码——– Ctrl+Alt+O 优化导入的类和包Alt+Insert 生成代码(如get,set方法,构造函数等) 或者右键（Generate）fori/sout/psvm + TabCtrl+Alt+T 生成try catch 或者 Alt+enterCTRL+ALT+T 把选中的代码放在 TRY{} IF{} ELSE{} 里Ctrl + O 重写方法Ctrl + I 实现方法Ctr+shift+U 大小写转化ALT+回车 导入包,自动修正ALT+/ 代码提示CTRL+J 自动代码Ctrl+Shift+J，整合两行为一行CTRL+空格 代码提示CTRL+SHIFT+SPACE 自动补全代码CTRL+ALT+L 格式化代码CTRL+ALT+I 自动缩进CTRL+ALT+O 优化导入的类和包ALT+INSERT 生成代码(如GET,SET方法,构造函数等)CTRL+E 最近更改的代码CTRL+ALT+SPACE 类名或接口名提示CTRL+P 方法参数提示CTRL+Q，可以看到当前方法的声明 Shift+F6 重构-重命名 (包、类、方法、变量、甚至注释等)Ctrl+Alt+V 提取变量 ———–查询快捷键——– Ctrl＋Shift＋Backspace可以跳转到上次编辑的地CTRL+ALT+ left/right 前后导航编辑过的地方ALT+7 靠左窗口显示当前文件的结构Ctrl+F12 浮动显示当前文件的结构ALT+F7 找到你的函数或者变量或者类的所有引用到的地方CTRL+ALT+F7 找到你的函数或者变量或者类的所有引用到的地方 Ctrl+Shift+Alt+N 查找类中的方法或变量双击SHIFT 在项目的所有目录查找文件Ctrl+N 查找类Ctrl+Shift+N 查找文件CTRL+G 定位行CTRL+F 在当前窗口查找文本CTRL+SHIFT+F 在指定窗口查找文本CTRL+R 在 当前窗口替换文本CTRL+SHIFT+R 在指定窗口替换文本ALT+SHIFT+C 查找修改的文件CTRL+E 最近打开的文件F3 向下查找关键字出现位置SHIFT+F3 向上一个关键字出现位置选中文本，按Alt+F3 ，高亮相同文本，F3逐个往下查找相同文本F4 查找变量来源 CTRL+SHIFT+O 弹出显示查找内容Ctrl+W 选中代码，连续按会有其他效果F2 或Shift+F2 高亮错误或警告快速定位Ctrl+Up/Down 光标跳转到第一行或最后一行下 Ctrl+B 快速打开光标处的类或方法CTRL+ALT+B 找所有的子类CTRL+SHIFT+B 找变量的类 Ctrl+Shift+上下键 上下移动代码Ctrl+Alt+ left/right 返回至上次浏览的位置Ctrl+X 剪切行Ctrl+D 复制行Ctrl+/ 或 Ctrl+Shift+/ 注释（// 或者/…/ ） Ctrl+H 显示类结构图Ctrl+Q 显示注释文档 Alt+F1 查找代码所在位置Alt+1 快速打开或隐藏工程面板 Alt+ left/right 切换代码视图ALT+ ↑/↓ 在方法间快速移动定位CTRL+ALT+ left/right 前后导航编辑过的地方Ctrl＋Shift＋Backspace可以跳转到上次编辑的地Alt+6 查找TODO 3.———————其他快捷键——————- SHIFT+ENTER 另起一行CTRL+Z 倒退(撤销)CTRL+SHIFT+Z 向前(取消撤销)CTRL+ALT+F12 资源管理器打开文件夹ALT+F1 查找文件所在目录位置SHIFT+ALT+INSERT 竖编辑模式CTRL+F4 关闭当前窗口Ctrl+Alt+V，可以引入变量。例如：new String(); 自动导入变量定义Ctrl+~，快速切换方案（界面外观、代码风格、快捷键映射等菜单） 4.————–svn快捷键————— ctrl+k 提交代码到SVNctrl+t 更新代码 5.————–调试快捷键—————其实常用的 就是F8 F7 F9 最值得一提的 就是Drop Frame 可以让运行过的代码从头再来alt+F8 debug时选中查看值Alt+Shift+F9，选择 DebugAlt+Shift+F10，选择 RunCtrl+Shift+F9，编译Ctrl+Shift+F8，查看断点 F7，步入Shift+F7，智能步入Alt+Shift+F7，强制步入F8，步过Shift+F8，步出Alt+Shift+F8，强制步过 Alt+F9，运行至光标处Ctrl+Alt+F9，强制运行至光标处F9，恢复程序Alt+F10，定位到断点 6.————–重构—————Ctrl+Alt+Shift+T，弹出重构菜单Shift+F6，重命名F6，移动F5，复制Alt+Delete，安全删除Ctrl+Alt+N，内联","link":"/2019/05/30/idea/"}],"tags":[{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"scoop","slug":"scoop","link":"/tags/scoop/"},{"name":"intellij-idea","slug":"intellij-idea","link":"/tags/intellij-idea/"}],"categories":[{"name":"深度学习","slug":"深度学习","link":"/categories/深度学习/"},{"name":"杂七杂八","slug":"杂七杂八","link":"/categories/杂七杂八/"}]}